{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DNet, SNet\n",
    "import data_generator as dg\n",
    "from data_generator import DenoisingDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time, os\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.nn.modules.loss import _Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum_squared_error(_Loss):  # PyTorch 0.4.1\n",
    "    \"\"\"\n",
    "    Definition: sum_squared_error = 1/2 * nn.MSELoss(reduction = 'sum')\n",
    "    The backward is defined as: input-target\n",
    "    \"\"\"\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='sum'):\n",
    "        super(sum_squared_error, self).__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # return torch.sum(torch.pow(input-target,2), (0,1,2,3)).div_(2)\n",
    "        return torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='sum').div_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain DNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "^_^-training data finished-^_^\n",
      "   1    0 / 1862 loss = 668.0828\n",
      "   1  100 / 1862 loss = 7.1625\n",
      "   1  200 / 1862 loss = 4.9611\n",
      "   1  300 / 1862 loss = 3.9745\n",
      "   1  400 / 1862 loss = 3.4851\n",
      "   1  500 / 1862 loss = 3.2205\n",
      "   1  600 / 1862 loss = 2.7894\n",
      "   1  700 / 1862 loss = 2.4739\n",
      "   1  800 / 1862 loss = 2.3594\n",
      "   1  900 / 1862 loss = 2.2900\n",
      "   1 1000 / 1862 loss = 2.1299\n",
      "   1 1100 / 1862 loss = 2.1909\n",
      "   1 1200 / 1862 loss = 2.1657\n",
      "   1 1300 / 1862 loss = 2.0969\n",
      "   1 1400 / 1862 loss = 1.9320\n",
      "   1 1500 / 1862 loss = 1.7944\n",
      "   1 1600 / 1862 loss = 1.9186\n",
      "   1 1700 / 1862 loss = 1.8426\n",
      "   1 1800 / 1862 loss = 1.7906\n",
      "epoch =    1 , loss = 600.3373 , time = 211.44 s\n",
      "^_^-training data finished-^_^\n",
      "   2    0 / 1862 loss = 1.9716\n",
      "   2  100 / 1862 loss = 1.9431\n",
      "   2  200 / 1862 loss = 1.7412\n",
      "   2  300 / 1862 loss = 1.7847\n",
      "   2  400 / 1862 loss = 1.8659\n",
      "   2  500 / 1862 loss = 1.7112\n",
      "   2  600 / 1862 loss = 1.6218\n",
      "   2  700 / 1862 loss = 1.8072\n",
      "   2  800 / 1862 loss = 1.7129\n",
      "   2  900 / 1862 loss = 1.8162\n",
      "   2 1000 / 1862 loss = 1.6540\n",
      "   2 1100 / 1862 loss = 1.7392\n",
      "   2 1200 / 1862 loss = 1.6595\n",
      "   2 1300 / 1862 loss = 1.7815\n",
      "   2 1400 / 1862 loss = 1.7850\n",
      "   2 1500 / 1862 loss = 1.7499\n",
      "   2 1600 / 1862 loss = 1.4791\n",
      "   2 1700 / 1862 loss = 1.6770\n",
      "   2 1800 / 1862 loss = 1.5519\n",
      "epoch =    2 , loss = 218.7579 , time = 211.60 s\n",
      "^_^-training data finished-^_^\n",
      "   3    0 / 1862 loss = 1.5770\n",
      "   3  100 / 1862 loss = 1.6802\n",
      "   3  200 / 1862 loss = 1.7134\n",
      "   3  300 / 1862 loss = 1.6020\n",
      "   3  400 / 1862 loss = 1.5575\n",
      "   3  500 / 1862 loss = 1.6308\n",
      "   3  600 / 1862 loss = 1.4554\n",
      "   3  700 / 1862 loss = 1.5278\n",
      "   3  800 / 1862 loss = 1.6359\n",
      "   3  900 / 1862 loss = 1.6884\n",
      "   3 1000 / 1862 loss = 1.5614\n",
      "   3 1100 / 1862 loss = 1.5698\n",
      "   3 1200 / 1862 loss = 1.7091\n",
      "   3 1300 / 1862 loss = 1.4274\n",
      "   3 1400 / 1862 loss = 1.4507\n",
      "   3 1500 / 1862 loss = 1.5467\n",
      "   3 1600 / 1862 loss = 1.3772\n",
      "   3 1700 / 1862 loss = 1.5305\n",
      "   3 1800 / 1862 loss = 1.5181\n",
      "epoch =    3 , loss = 200.8417 , time = 210.59 s\n",
      "^_^-training data finished-^_^\n",
      "   4    0 / 1862 loss = 1.4985\n",
      "   4  100 / 1862 loss = 1.7179\n",
      "   4  200 / 1862 loss = 1.4898\n",
      "   4  300 / 1862 loss = 1.4294\n",
      "   4  400 / 1862 loss = 1.4925\n",
      "   4  500 / 1862 loss = 1.4456\n",
      "   4  600 / 1862 loss = 1.4967\n",
      "   4  700 / 1862 loss = 1.4669\n",
      "   4  800 / 1862 loss = 1.5203\n",
      "   4  900 / 1862 loss = 1.5079\n",
      "   4 1000 / 1862 loss = 1.4285\n",
      "   4 1100 / 1862 loss = 1.4887\n",
      "   4 1200 / 1862 loss = 1.5108\n",
      "   4 1300 / 1862 loss = 1.3750\n",
      "   4 1400 / 1862 loss = 1.4839\n",
      "   4 1500 / 1862 loss = 3.6817\n",
      "   4 1600 / 1862 loss = 1.5943\n",
      "   4 1700 / 1862 loss = 1.4443\n",
      "   4 1800 / 1862 loss = 1.5760\n",
      "epoch =    4 , loss = 196.7764 , time = 210.76 s\n",
      "^_^-training data finished-^_^\n",
      "   5    0 / 1862 loss = 1.3857\n",
      "   5  100 / 1862 loss = 1.5057\n",
      "   5  200 / 1862 loss = 1.4582\n",
      "   5  300 / 1862 loss = 1.5317\n",
      "   5  400 / 1862 loss = 1.4807\n",
      "   5  500 / 1862 loss = 1.4255\n",
      "   5  600 / 1862 loss = 1.3157\n",
      "   5  700 / 1862 loss = 1.4160\n",
      "   5  800 / 1862 loss = 1.5919\n",
      "   5  900 / 1862 loss = 1.5250\n",
      "   5 1000 / 1862 loss = 1.5688\n",
      "   5 1100 / 1862 loss = 1.5215\n",
      "   5 1200 / 1862 loss = 1.4430\n",
      "   5 1300 / 1862 loss = 1.3608\n",
      "   5 1400 / 1862 loss = 1.3199\n",
      "   5 1500 / 1862 loss = 1.5797\n",
      "   5 1600 / 1862 loss = 1.4755\n",
      "   5 1700 / 1862 loss = 1.4875\n",
      "   5 1800 / 1862 loss = 1.5412\n",
      "epoch =    5 , loss = 184.8386 , time = 211.59 s\n",
      "^_^-training data finished-^_^\n",
      "   6    0 / 1862 loss = 1.3967\n",
      "   6  100 / 1862 loss = 1.2848\n",
      "   6  200 / 1862 loss = 1.2747\n",
      "   6  300 / 1862 loss = 1.4278\n",
      "   6  400 / 1862 loss = 1.4296\n",
      "   6  500 / 1862 loss = 1.3530\n",
      "   6  600 / 1862 loss = 1.5089\n",
      "   6  700 / 1862 loss = 1.3324\n",
      "   6  800 / 1862 loss = 1.4674\n",
      "   6  900 / 1862 loss = 1.5106\n",
      "   6 1000 / 1862 loss = 1.3792\n",
      "   6 1100 / 1862 loss = 1.2632\n",
      "   6 1200 / 1862 loss = 1.4065\n",
      "   6 1300 / 1862 loss = 1.3513\n",
      "   6 1400 / 1862 loss = 1.4931\n",
      "   6 1500 / 1862 loss = 1.3294\n",
      "   6 1600 / 1862 loss = 1.5200\n",
      "   6 1700 / 1862 loss = 1.3242\n",
      "   6 1800 / 1862 loss = 1.4558\n",
      "epoch =    6 , loss = 178.9252 , time = 211.75 s\n",
      "^_^-training data finished-^_^\n",
      "   7    0 / 1862 loss = 1.5693\n",
      "   7  100 / 1862 loss = 1.4876\n",
      "   7  200 / 1862 loss = 1.4444\n",
      "   7  300 / 1862 loss = 1.2861\n",
      "   7  400 / 1862 loss = 1.4549\n",
      "   7  500 / 1862 loss = 1.2032\n",
      "   7  600 / 1862 loss = 1.4050\n",
      "   7  700 / 1862 loss = 1.2753\n",
      "   7  800 / 1862 loss = 1.4835\n",
      "   7  900 / 1862 loss = 1.5554\n",
      "   7 1000 / 1862 loss = 1.3857\n",
      "   7 1100 / 1862 loss = 1.3943\n",
      "   7 1200 / 1862 loss = 1.3919\n",
      "   7 1300 / 1862 loss = 1.3449\n",
      "   7 1400 / 1862 loss = 1.4424\n",
      "   7 1500 / 1862 loss = 1.4915\n",
      "   7 1600 / 1862 loss = 1.2970\n",
      "   7 1700 / 1862 loss = 1.4205\n",
      "   7 1800 / 1862 loss = 1.1445\n",
      "epoch =    7 , loss = 175.2197 , time = 211.62 s\n",
      "^_^-training data finished-^_^\n",
      "   8    0 / 1862 loss = 1.3925\n",
      "   8  100 / 1862 loss = 1.3502\n",
      "   8  200 / 1862 loss = 1.4222\n",
      "   8  300 / 1862 loss = 1.3266\n",
      "   8  400 / 1862 loss = 1.3344\n",
      "   8  500 / 1862 loss = 1.3568\n",
      "   8  600 / 1862 loss = 1.3860\n",
      "   8  700 / 1862 loss = 1.3709\n",
      "   8  800 / 1862 loss = 1.3868\n",
      "   8  900 / 1862 loss = 1.4702\n",
      "   8 1000 / 1862 loss = 1.3100\n",
      "   8 1100 / 1862 loss = 1.4510\n",
      "   8 1200 / 1862 loss = 1.3191\n",
      "   8 1300 / 1862 loss = 1.1945\n",
      "   8 1400 / 1862 loss = 1.3042\n",
      "   8 1500 / 1862 loss = 1.3374\n",
      "   8 1600 / 1862 loss = 1.3886\n",
      "   8 1700 / 1862 loss = 1.3935\n",
      "   8 1800 / 1862 loss = 1.2367\n",
      "epoch =    8 , loss = 172.8514 , time = 212.17 s\n",
      "^_^-training data finished-^_^\n",
      "   9    0 / 1862 loss = 1.1571\n",
      "   9  100 / 1862 loss = 1.5500\n",
      "   9  200 / 1862 loss = 1.3682\n",
      "   9  300 / 1862 loss = 1.3917\n",
      "   9  400 / 1862 loss = 1.3917\n",
      "   9  500 / 1862 loss = 1.3822\n",
      "   9  600 / 1862 loss = 1.3162\n",
      "   9  700 / 1862 loss = 1.3701\n",
      "   9  800 / 1862 loss = 1.4474\n",
      "   9  900 / 1862 loss = 1.2868\n",
      "   9 1000 / 1862 loss = 1.2726\n",
      "   9 1100 / 1862 loss = 1.2561\n",
      "   9 1200 / 1862 loss = 1.3497\n",
      "   9 1300 / 1862 loss = 1.4397\n",
      "   9 1400 / 1862 loss = 1.2824\n",
      "   9 1500 / 1862 loss = 1.2917\n",
      "   9 1600 / 1862 loss = 1.2025\n",
      "   9 1700 / 1862 loss = 1.4242\n",
      "   9 1800 / 1862 loss = 1.5132\n",
      "epoch =    9 , loss = 173.1148 , time = 212.12 s\n",
      "^_^-training data finished-^_^\n",
      "  10    0 / 1862 loss = 1.4444\n",
      "  10  100 / 1862 loss = 1.4498\n",
      "  10  200 / 1862 loss = 1.3526\n",
      "  10  300 / 1862 loss = 1.3861\n",
      "  10  400 / 1862 loss = 1.3578\n",
      "  10  500 / 1862 loss = 1.3527\n",
      "  10  600 / 1862 loss = 1.3464\n",
      "  10  700 / 1862 loss = 1.3320\n",
      "  10  800 / 1862 loss = 1.2412\n",
      "  10  900 / 1862 loss = 1.2387\n",
      "  10 1000 / 1862 loss = 1.3084\n",
      "  10 1100 / 1862 loss = 1.3731\n",
      "  10 1200 / 1862 loss = 1.2097\n",
      "  10 1300 / 1862 loss = 1.2544\n",
      "  10 1400 / 1862 loss = 1.4215\n",
      "  10 1500 / 1862 loss = 1.3721\n",
      "  10 1600 / 1862 loss = 1.4785\n",
      "  10 1700 / 1862 loss = 1.2304\n",
      "  10 1800 / 1862 loss = 1.3767\n",
      "epoch =   10 , loss = 170.0224 , time = 211.73 s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "n_epoch = 150\n",
    "sigma = 25\n",
    "lr = 1e-3\n",
    "data_dir = './data/Train400'\n",
    "model_dir = 'models'\n",
    "model_name = 'DNet_sigma=25_1.pth'\n",
    "\n",
    "model = DNet()\n",
    "\n",
    "model.train()\n",
    "criterion = sum_squared_error()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.2)  # learning rates\n",
    "for epoch in range(n_epoch):\n",
    "    x = dg.datagenerator(data_dir=data_dir).astype('float32')/255.0\n",
    "    x = torch.from_numpy(x.transpose((0, 3, 1, 2)))\n",
    "    dataset = DenoisingDataset(x, sigma)\n",
    "    loader = DataLoader(dataset=dataset, num_workers=4, drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    n_count=0\n",
    "    for cnt, batch_yx in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        if cuda:\n",
    "            batch_original, batch_noise = batch_yx[1].cuda(), batch_yx[0].cuda()\n",
    "        loss = criterion(batch_noise- model(batch_noise), batch_original)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if cnt%100 == 0:\n",
    "            print('%4d %4d / %4d loss = %2.4f' % (epoch+1, cnt, x.size(0)//batch_size, loss.item()/batch_size))\n",
    "        n_count +=1\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('epoch = %4d , loss = %4.4f , time = %4.2f s' % (epoch+1, epoch_loss/n_count, elapsed_time))\n",
    "    torch.save(model, os.path.join(model_dir, model_name))\n",
    "\n",
    "torch.save(model, os.path.join(model_dir, model_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain SNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "^_^-training data finished-^_^\n",
      "   1    0 / 1862 loss = 146.3578\n",
      "   1  100 / 1862 loss = 24.3115\n",
      "   1  200 / 1862 loss = 16.6896\n",
      "   1  300 / 1862 loss = 14.8828\n",
      "   1  400 / 1862 loss = 14.9912\n",
      "   1  500 / 1862 loss = 12.3780\n",
      "   1  600 / 1862 loss = 12.3868\n",
      "   1  700 / 1862 loss = 12.3291\n",
      "   1  800 / 1862 loss = 11.4862\n",
      "   1  900 / 1862 loss = 10.6100\n",
      "   1 1000 / 1862 loss = 10.7344\n",
      "   1 1100 / 1862 loss = 11.0920\n",
      "   1 1200 / 1862 loss = 10.7167\n",
      "   1 1300 / 1862 loss = 10.7239\n",
      "   1 1400 / 1862 loss = 9.9885\n",
      "   1 1500 / 1862 loss = 10.2467\n",
      "   1 1600 / 1862 loss = 9.9037\n",
      "   1 1700 / 1862 loss = 9.7338\n",
      "   1 1800 / 1862 loss = 9.6482\n",
      "epoch =    1 , loss = 1750.1406 , time = 248.19 s\n",
      "^_^-training data finished-^_^\n",
      "   2    0 / 1862 loss = 9.3908\n",
      "   2  100 / 1862 loss = 10.1689\n",
      "   2  200 / 1862 loss = 10.0545\n",
      "   2  300 / 1862 loss = 9.5249\n",
      "   2  400 / 1862 loss = 9.1737\n",
      "   2  500 / 1862 loss = 9.6369\n",
      "   2  600 / 1862 loss = 9.0532\n",
      "   2  700 / 1862 loss = 8.8624\n",
      "   2  800 / 1862 loss = 13.9905\n",
      "   2  900 / 1862 loss = 11.2479\n",
      "   2 1000 / 1862 loss = 9.5484\n",
      "   2 1100 / 1862 loss = 9.3235\n",
      "   2 1200 / 1862 loss = 11.7386\n",
      "   2 1300 / 1862 loss = 9.7246\n",
      "   2 1400 / 1862 loss = 9.6596\n",
      "   2 1500 / 1862 loss = 9.5580\n",
      "   2 1600 / 1862 loss = 8.4983\n",
      "   2 1700 / 1862 loss = 8.6893\n",
      "   2 1800 / 1862 loss = 9.4546\n",
      "epoch =    2 , loss = 1279.6752 , time = 248.69 s\n",
      "^_^-training data finished-^_^\n",
      "   3    0 / 1862 loss = 8.8870\n",
      "   3  100 / 1862 loss = 9.5550\n",
      "   3  200 / 1862 loss = 8.7394\n",
      "   3  300 / 1862 loss = 8.3913\n",
      "   3  400 / 1862 loss = 9.1361\n",
      "   3  500 / 1862 loss = 9.1369\n",
      "   3  600 / 1862 loss = 9.1240\n",
      "   3  700 / 1862 loss = 9.1002\n",
      "   3  800 / 1862 loss = 8.6005\n",
      "   3  900 / 1862 loss = 8.5715\n",
      "   3 1000 / 1862 loss = 8.5542\n",
      "   3 1100 / 1862 loss = 8.6194\n",
      "   3 1200 / 1862 loss = 8.3937\n",
      "   3 1300 / 1862 loss = 9.2265\n",
      "   3 1400 / 1862 loss = 8.0756\n",
      "   3 1500 / 1862 loss = 8.6902\n",
      "   3 1600 / 1862 loss = 8.4727\n",
      "   3 1700 / 1862 loss = 8.7433\n",
      "   3 1800 / 1862 loss = 8.3946\n",
      "epoch =    3 , loss = 1117.0228 , time = 248.21 s\n",
      "^_^-training data finished-^_^\n",
      "   4    0 / 1862 loss = 8.4962\n",
      "   4  100 / 1862 loss = 8.3622\n",
      "   4  200 / 1862 loss = 7.6011\n",
      "   4  300 / 1862 loss = 8.1529\n",
      "   4  400 / 1862 loss = 8.2315\n",
      "   4  500 / 1862 loss = 8.6219\n",
      "   4  600 / 1862 loss = 9.1058\n",
      "   4  700 / 1862 loss = 7.8296\n",
      "   4  800 / 1862 loss = 8.4714\n",
      "   4  900 / 1862 loss = 8.1549\n",
      "   4 1000 / 1862 loss = 8.0914\n",
      "   4 1100 / 1862 loss = 8.1054\n",
      "   4 1200 / 1862 loss = 7.9972\n",
      "   4 1300 / 1862 loss = 8.2476\n",
      "   4 1400 / 1862 loss = 8.0279\n",
      "   4 1500 / 1862 loss = 8.2530\n",
      "   4 1600 / 1862 loss = 8.1649\n",
      "   4 1700 / 1862 loss = 8.2069\n",
      "   4 1800 / 1862 loss = 7.7954\n",
      "epoch =    4 , loss = 1069.1399 , time = 248.07 s\n",
      "^_^-training data finished-^_^\n",
      "   5    0 / 1862 loss = 8.2846\n",
      "   5  100 / 1862 loss = 8.0818\n",
      "   5  200 / 1862 loss = 7.6526\n",
      "   5  300 / 1862 loss = 8.0880\n",
      "   5  400 / 1862 loss = 8.4463\n",
      "   5  500 / 1862 loss = 7.9011\n",
      "   5  600 / 1862 loss = 8.2447\n",
      "   5  700 / 1862 loss = 8.0878\n",
      "   5  800 / 1862 loss = 7.4064\n",
      "   5  900 / 1862 loss = 8.0960\n",
      "   5 1000 / 1862 loss = 8.0600\n",
      "   5 1100 / 1862 loss = 8.4176\n",
      "   5 1200 / 1862 loss = 7.8167\n",
      "   5 1300 / 1862 loss = 8.1589\n",
      "   5 1400 / 1862 loss = 8.1994\n",
      "   5 1500 / 1862 loss = 8.0199\n",
      "   5 1600 / 1862 loss = 7.8520\n",
      "   5 1700 / 1862 loss = 8.3738\n",
      "   5 1800 / 1862 loss = 7.7961\n",
      "epoch =    5 , loss = 1035.8343 , time = 247.69 s\n",
      "^_^-training data finished-^_^\n",
      "   6    0 / 1862 loss = 7.9559\n",
      "   6  100 / 1862 loss = 8.3554\n",
      "   6  200 / 1862 loss = 8.5624\n",
      "   6  300 / 1862 loss = 8.2369\n",
      "   6  400 / 1862 loss = 7.7789\n",
      "   6  500 / 1862 loss = 7.9386\n",
      "   6  600 / 1862 loss = 8.1758\n",
      "   6  700 / 1862 loss = 7.4082\n",
      "   6  800 / 1862 loss = 7.4930\n",
      "   6  900 / 1862 loss = 7.4301\n",
      "   6 1000 / 1862 loss = 7.8077\n",
      "   6 1100 / 1862 loss = 7.5252\n",
      "   6 1200 / 1862 loss = 7.4104\n",
      "   6 1300 / 1862 loss = 7.8826\n",
      "   6 1400 / 1862 loss = 7.3374\n",
      "   6 1500 / 1862 loss = 7.8428\n",
      "   6 1600 / 1862 loss = 7.6787\n",
      "   6 1700 / 1862 loss = 7.7269\n",
      "   6 1800 / 1862 loss = 7.5480\n",
      "epoch =    6 , loss = 1012.4005 , time = 248.59 s\n",
      "^_^-training data finished-^_^\n",
      "   7    0 / 1862 loss = 8.2863\n",
      "   7  100 / 1862 loss = 8.0120\n",
      "   7  200 / 1862 loss = 7.6007\n",
      "   7  300 / 1862 loss = 7.9437\n",
      "   7  400 / 1862 loss = 7.6038\n",
      "   7  500 / 1862 loss = 7.5054\n",
      "   7  600 / 1862 loss = 8.2729\n",
      "   7  700 / 1862 loss = 7.6109\n",
      "   7  800 / 1862 loss = 8.3053\n",
      "   7  900 / 1862 loss = 7.6581\n",
      "   7 1000 / 1862 loss = 7.4874\n",
      "   7 1100 / 1862 loss = 7.9926\n",
      "   7 1200 / 1862 loss = 7.6847\n",
      "   7 1300 / 1862 loss = 7.8138\n",
      "   7 1400 / 1862 loss = 7.2912\n",
      "   7 1500 / 1862 loss = 7.3376\n",
      "   7 1600 / 1862 loss = 7.2104\n",
      "   7 1700 / 1862 loss = 7.6174\n",
      "   7 1800 / 1862 loss = 8.0536\n",
      "epoch =    7 , loss = 998.2248 , time = 249.36 s\n",
      "^_^-training data finished-^_^\n",
      "   8    0 / 1862 loss = 7.8001\n",
      "   8  100 / 1862 loss = 7.6013\n",
      "   8  200 / 1862 loss = 7.6083\n",
      "   8  300 / 1862 loss = 7.3288\n",
      "   8  400 / 1862 loss = 7.6070\n",
      "   8  500 / 1862 loss = 8.6712\n",
      "   8  600 / 1862 loss = 7.6584\n",
      "   8  700 / 1862 loss = 7.3163\n",
      "   8  800 / 1862 loss = 7.8067\n",
      "   8  900 / 1862 loss = 7.6617\n",
      "   8 1000 / 1862 loss = 7.6003\n",
      "   8 1100 / 1862 loss = 7.8133\n",
      "   8 1200 / 1862 loss = 7.7098\n",
      "   8 1300 / 1862 loss = 7.7918\n",
      "   8 1400 / 1862 loss = 7.7119\n",
      "   8 1500 / 1862 loss = 7.4565\n",
      "   8 1600 / 1862 loss = 7.3432\n",
      "   8 1700 / 1862 loss = 8.6728\n",
      "   8 1800 / 1862 loss = 7.6940\n",
      "epoch =    8 , loss = 985.5792 , time = 248.84 s\n",
      "^_^-training data finished-^_^\n",
      "   9    0 / 1862 loss = 7.5977\n",
      "   9  100 / 1862 loss = 7.4780\n",
      "   9  200 / 1862 loss = 7.2817\n",
      "   9  300 / 1862 loss = 7.7264\n",
      "   9  400 / 1862 loss = 7.9390\n",
      "   9  500 / 1862 loss = 7.8709\n",
      "   9  600 / 1862 loss = 7.0599\n",
      "   9  700 / 1862 loss = 7.5113\n",
      "   9  800 / 1862 loss = 7.4826\n",
      "   9  900 / 1862 loss = 7.1298\n",
      "   9 1000 / 1862 loss = 7.7281\n",
      "   9 1100 / 1862 loss = 7.2124\n",
      "   9 1200 / 1862 loss = 7.1648\n",
      "   9 1300 / 1862 loss = 8.1833\n",
      "   9 1400 / 1862 loss = 7.9233\n",
      "   9 1500 / 1862 loss = 7.3704\n",
      "   9 1600 / 1862 loss = 7.5580\n",
      "   9 1700 / 1862 loss = 7.1735\n",
      "   9 1800 / 1862 loss = 6.8655\n",
      "epoch =    9 , loss = 976.6118 , time = 248.74 s\n",
      "^_^-training data finished-^_^\n",
      "  10    0 / 1862 loss = 7.0053\n",
      "  10  100 / 1862 loss = 7.6683\n",
      "  10  200 / 1862 loss = 8.1421\n",
      "  10  300 / 1862 loss = 7.6822\n",
      "  10  400 / 1862 loss = 7.7472\n",
      "  10  500 / 1862 loss = 7.0353\n",
      "  10  600 / 1862 loss = 7.0484\n",
      "  10  700 / 1862 loss = 7.6905\n",
      "  10  800 / 1862 loss = 7.8723\n",
      "  10  900 / 1862 loss = 7.6218\n",
      "  10 1000 / 1862 loss = 6.9206\n",
      "  10 1100 / 1862 loss = 7.4344\n",
      "  10 1200 / 1862 loss = 7.7326\n",
      "  10 1300 / 1862 loss = 7.7591\n",
      "  10 1400 / 1862 loss = 7.1307\n",
      "  10 1500 / 1862 loss = 7.3766\n",
      "  10 1600 / 1862 loss = 7.8926\n",
      "  10 1700 / 1862 loss = 7.4174\n",
      "  10 1800 / 1862 loss = 7.5918\n",
      "epoch =   10 , loss = 970.4102 , time = 248.61 s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "n_epoch = 10\n",
    "sigma = 25\n",
    "lr = 1e-3\n",
    "data_dir = './data/Train400'\n",
    "model_dir = 'models'\n",
    "model_name = 'DNet_sigma=25_1.pth'\n",
    "save_name_s = 'SNet_sigma=25_1.pth'\n",
    "\n",
    "SNet_model = SNet()\n",
    "DNet_model = torch.load(os.path.join(model_dir, model_name))\n",
    "\n",
    "DNet_model.eval()\n",
    "SNet_model.train()\n",
    "\n",
    "criterion = sum_squared_error()\n",
    "\n",
    "if cuda:\n",
    "    DNet_model = DNet_model.cuda()\n",
    "    SNet_model = SNet_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(DNet_model.parameters(), lr=lr)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.2)  # learning rates\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    x = dg.datagenerator(data_dir=data_dir).astype('float32')/255.0\n",
    "    x = torch.from_numpy(x.transpose((0, 3, 1, 2)))\n",
    "    dataset = DenoisingDataset(x, sigma)\n",
    "    loader = DataLoader(dataset=dataset, num_workers=4, drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    n_count=0\n",
    "    \n",
    "    for cnt, batch_yx in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        if cuda:\n",
    "            batch_original, batch_noise = batch_yx[1].cuda(), batch_yx[0].cuda()\n",
    "        \n",
    "        residual = DNet_model(batch_noise)\n",
    "        denoised = batch_noise - residual\n",
    "        \n",
    "        residual = 1.55*(residual+0.5)-0.8\n",
    "        structure = SNet_model(residual, denoised)\n",
    "        target = 1.8*(batch_original-denoised+0.5)-0.8\n",
    "        \n",
    "        loss = criterion(structure, target)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if cnt%100 == 0:\n",
    "            print('%4d %4d / %4d loss = %2.4f' % (epoch+1, cnt, x.size(0)//batch_size, loss.item()/batch_size))\n",
    "        n_count +=1\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('epoch = %4d , loss = %4.4f , time = %4.2f s' % (epoch+1, epoch_loss/n_count, elapsed_time))\n",
    "    torch.save(SNet_model, os.path.join(model_dir, save_name_s))\n",
    "\n",
    "torch.save(SNet_model, os.path.join(model_dir, save_name_s))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35(env1_1)",
   "language": "python",
   "name": "env1_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
